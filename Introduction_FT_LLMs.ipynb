{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Fintuning LLMs using Huggingface\n",
    "> Workshop by Tree at #Cosin2025!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rapidly evolving field of NLP and LLMs, Hugging Face ü§ó provides access to state-of-the-art AI models and making it easier for developers, researchers, and organizations to integrate advanced NLP capabilities into their applications.\n",
    "\n",
    "The most important parts of the HF ecosystem include: \n",
    "\n",
    "- ü§ó Model Hub ‚Äì A repository of thousands of open-source ML models.\n",
    "- ü§ó Datasets ‚Äì A vast collection of high-quality NLP datasets.\n",
    "- ü§ó Tokenizers ‚Äì Efficient and customizable tokenization methods for text processing.\n",
    "- ü§ó Accelerate ‚Äì Tools for optimizing and scaling model training.\n",
    "- ü§ó PEFT (Parameter-Efficient Fine-Tuning) ‚Äì Techniques like LoRA for efficient model adaptation.\n",
    "- ü§ó Spaces ‚Äì A platform to deploy and share AI applications.\n",
    "\n",
    "and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to provide you with a quickstart and a basic overview on how to use the ecosystem. We will load a model, learn how to fine tune a model using Parameter-Efficient Fine-Tuning (PEFT) methods and evaluate them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is influenced by the following hugging face notebook: https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Before you begin, make sure you have all the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers bitsandbytes accelerate peft datasets torch torchinfo matplotlib pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run different functions with a tensorflow backend. But in the scope of this notebook we focus on the pytorch backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline() is the easiest and fastest way to use a pretrained model for inference. You can use the pipeline() out-of-the-box for many tasks across different modalities, some of which are shown in the table below:\n",
    "\n",
    "| **Task**                     | **Description**                                                                                              | **Modality**    | **Pipeline identifier**                       |\n",
    "|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|\n",
    "| Text classification          | assign a label to a given sequence of text                                                                   | NLP             | pipeline(task=‚Äúsentiment-analysis‚Äù)           |\n",
    "| Text generation              | generate text given a prompt                                                                                 | NLP             | pipeline(task=‚Äútext-generation‚Äù)              |\n",
    "| Summarization                | generate a summary of a sequence of text or document                                                         | NLP             | pipeline(task=‚Äúsummarization‚Äù)                |\n",
    "| Image classification         | assign a label to an image                                                                                   | Computer vision | pipeline(task=‚Äúimage-classification‚Äù)         |\n",
    "| Image segmentation           | assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation) | Computer vision | pipeline(task=‚Äúimage-segmentation‚Äù)           |\n",
    "| Object detection             | predict the bounding boxes and classes of objects in an image                                                | Computer vision | pipeline(task=‚Äúobject-detection‚Äù)             |\n",
    "| Audio classification         | assign a label to some audio data                                                                            | Audio           | pipeline(task=‚Äúaudio-classification‚Äù)         |\n",
    "| Automatic speech recognition | transcribe speech into text                                                                                  | Audio           | pipeline(task=‚Äúautomatic-speech-recognition‚Äù) |\n",
    "| Visual question answering    | answer a question about the image, given an image and a question                                             | Multimodal      | pipeline(task=‚Äúvqa‚Äù)                          |\n",
    "| Document question answering  | answer a question about a document, given an image and a question                                            | Multimodal      | pipeline(task=\"document-question-answering\")  |\n",
    "| Image captioning             | generate a caption for a given image                                                                         | Multimodal      | pipeline(task=\"image-to-text\")                |\n",
    "\n",
    "**Note:** For a complete list of available tasks, check out the [pipeline API reference](https://huggingface.co/docs/transformers/main/en/./main_classes/pipelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline() downloads and caches a default pretrained model and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"We are very happy to show you the ü§ó Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the task it takes the default model for the pipeline and solves the task. To find out what model was used use the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.model.name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As displayed in the table above, we can use the pipeline on a variety of tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator(\"Cosin is a conferenc at \", max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TASK\n",
    "\n",
    "Use the pipeline function to generate text using the pythia 70M model. \n",
    "\n",
    "1. Load the model using the `AutoModelForCausalLM` from `transformers`.\n",
    "2. Load the tokenizer usint the `AutoTokenizer` from `transformers`. \n",
    "3. Define the pipeline with the model and tokenizer.\n",
    "4. Use the generator to generate `150` tokens after the given prompt.\n",
    "5. Print out the generated Text.\n",
    "\n",
    "**Note:** The 70m Model-Size of Pythia is quite small. The Llama models start at 8B parameters: \n",
    "- Llama 3.1 8B\n",
    "- Llama 3.1 70B\n",
    "- Llama 3.1 405B\n",
    "\n",
    "To make this notebook accessible we use small models. If you want, feel free to change the models to bigger ones from HF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The best thing about the CCC-CH is:\\n\"\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "\n",
    "### IMPLEMENT YOUR SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 AutoClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the `AutoModelForCausalLM` and `AutoTokenizer` classes work together to power the `pipeline` you used above. An `AutoClass` is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate `AutoClass` for your task and it's associated preprocessing class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AutoTokenizer` for example loads the correct tokenizer for the given model and we can observe the encodings for different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "pythia = \"EleutherAI/pythia-70m\"\n",
    "gpt2 = \"gpt2\"\n",
    "falcon_7b = \"tiiuae/falcon-7b\"\n",
    "\n",
    "pythia_tokenizer = AutoTokenizer.from_pretrained(pythia)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2)\n",
    "falcon_7b_tokenizer = AutoTokenizer.from_pretrained(falcon_7b)\n",
    "\n",
    "\n",
    "text = \"This text will be encoded by different tokenizers to show the differences in tokenization.\"\n",
    "\n",
    "print(f\"Pythia tokenizer: {pythia_tokenizer(text)}\")\n",
    "print(f\"GPT-2 tokenizer: {gpt2_tokenizer(text)}\")\n",
    "print(f\"Falcon-7B tokenizer: {falcon_7b_tokenizer(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary containing:\n",
    "- `input_ids`: numerical representations of your tokens.\n",
    "- `attention_mask`: indicates which tokens should be attended to.\n",
    "\n",
    "\n",
    "It is important to use the correct tokenizer per model. The `AutoClass` helps us to load the correct tokenizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the pipeline function, we can also load different models using the `AutoModel` functions. Similar to the `AutoTkenizer` we only need to provide the model name or HF path. When generating text we use the `AutoModelForCausalLM`function to load LLMs. Causal Language Modeling refers to next token prediction one at a time only using past tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading we can inspect the model. \n",
    "\n",
    "**Note:** \"GPTNeoX\" is the old/internatl name for the Pythia models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to print the model and its configuration\n",
    "# print(model)\n",
    "# print(model.config)\n",
    "\n",
    "## alternative use torchinfo and adjust the depth to see the model architecture\n",
    "from torchinfo import summary\n",
    "summary(model, depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model we need the tokenizer we defined earlier again and we show how we can use a list of prompts to generate outputs for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"A hackerspace is\",\n",
    "          \"Access Granted!\"]\n",
    "\n",
    "\n",
    "# we need to define a padding token for the tokenizer to appand to the prompts\n",
    "pythia_tokenizer.pad_token = pythia_tokenizer.eos_token\n",
    "\n",
    "# set padding side left\n",
    "pythia_tokenizer.padding_side = \"left\"\n",
    "\n",
    "tokenized_batch = pythia_tokenizer(\n",
    "    prompt,\n",
    "    padding=True,\n",
    "    truncation=True,  # if the prompt would be to long it would be truncated\n",
    "    max_length=512,  # we set this so that the model is not confronted with to large input\n",
    "    return_tensors=\"pt\",  # return PyTorch tensors as encoding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can give the encoded prompts to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**tokenized_batch, max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After decoding we get back the generated texts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = pythia_tokenizer.batch_decode(\n",
    "    output, skip_special_tokens=True)\n",
    "generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Saving Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the fine tuning (what we will cover later) it is important to save and load the model. For that we need a path and the `PreTrainedModel.save_pretrained()` function. \n",
    "To save the model properly, we also save the tokenizer. This allows to use the models without internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./my_saved_pythia_model\"\n",
    "pythia_tokenizer.save_pretrained(save_dir)\n",
    "model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a folder with the following structure:\n",
    "```\n",
    "my_saved_pythia_model/\n",
    "‚îÇ‚îÄ‚îÄ config.json\n",
    "‚îÇ‚îÄ‚îÄ pytorch_model.bin\n",
    "‚îÇ‚îÄ‚îÄ special_tokens_map.json\n",
    "‚îÇ‚îÄ‚îÄ tokenizer_config.json\n",
    "‚îÇ‚îÄ‚îÄ tokenizer.json\n",
    "```\n",
    "\n",
    "Now we can load the model from our saved files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./my_saved_pythia_model\"\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will observe how HF uses the PyTorch backend to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are a standard [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) so you can use them in any typical training loop. While you can write your own training loop, ü§ñ Transformers provides a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for PyTorch, which contains the basic training loop and adds additional functionality for features like distributed training, mixed precision, and more.\n",
    "\n",
    "Depending on your task, you'll typically pass the following parameters to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer):\n",
    "\n",
    "1. A [PreTrainedModel](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel) or a [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) contains the model hyperparameters you can change like learning rate, batch size, and the number of epochs to train for. The default values are used if you don't specify any training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"path/to/save/folder/\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A preprocessing class like a tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "\n",
    "# set padding token and token side left\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "for split in [\"train\", \"test\", \"validation\"]:\n",
    "    print(f\"{split} size:\", len(dataset[split]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a function to tokenize the dataset and apply it over the entire dataset with [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. A [DataCollatorForLanguageModeling](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling) to create a batch of examples from your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now gather all these classes in [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're ready, call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to start training. This might take a wile. Feel free to skip this step. We will learn faster training methods afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard training of such a small dataset takes really long. So you might skip that. Since not every one is able to train the full model on thier hardware there are other methods to fine tune the model in a more efficient way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter-Efficient Fine-Tuning (PEFT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT offers parameter-efficient methods for finetuning large pretrained models. The traditional paradigm is to finetune all of a model‚Äôs parameters for each downstream task, but this is becoming exceedingly costly and impractical because of the enormous number of parameters in models today. Instead, it is more efficient to train a smaller number of prompt parameters or use a reparametrization method like low-rank adaptation (LoRA) to reduce the number of trainable parameters.\n",
    "\n",
    "In this notebook we focus on low-rank adaptation (LoRA) but there are much more aproaches and technics in the realm of PEFT. The following graphic displays a categorisation and a overview on the PEFT methods. Keep in mind that this is from a paper from 2023 by Lialin et al. called \"[Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)\". Since the paper there are even more methods and approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-uploads.huggingface.co/production/uploads/666b9ef5e6c60b6fc4156675/dz0AdSqt4QP7iRjpiXDE1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understanding LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-Rank Adaptation (LoRA) is a technique used to fine-tune large language models efficiently by reducing the number of trainable parameters. The core idea is to approximate the weight updates using low-rank matrices:\n",
    "\n",
    "**0. Neural Network definition**\n",
    "\n",
    "In a standard neural network layer, the transformation of an input vector $x$ is performed using a weight matrix $W$ and a bias vector $b$:\n",
    "\n",
    "$$\n",
    "y = W x + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x \\in \\mathbb{R}^{d}$ is the input feature vector,\n",
    "- $W \\in \\mathbb{R}^{d \\times k}$ is the weight matrix that maps the input to an output of dimension $k$,\n",
    "- $b \\in \\mathbb{R}^{k}$ is the bias vector, and\n",
    "- $y \\in \\mathbb{R}^{k}$ is the output of the layer.\n",
    "\n",
    "This operation is repeated in every layer of a deep neural network. During training, the weight matrix $W$ is updated using gradient-based optimization to minimize a loss function.\n",
    "\n",
    "**1. Original Weight Matrix**\n",
    "\n",
    "Let $ W \\in \\mathbb{R}^{d \\times k} $ be the original weight matrix of a neural network layer, where $ d $ is the input dimension and $ k $ is the output dimension.\n",
    "\n",
    "**2. Low-Rank Decomposition**\n",
    "\n",
    "Instead of updating the entire weight matrix $ W $, LoRA decomposes the update into two smaller matrices:\n",
    "\n",
    "$$\n",
    "\\Delta W = A B\n",
    "$$\n",
    "\n",
    "where $ A \\in \\mathbb{R}^{d \\times r} $ and $ B \\in \\mathbb{R}^{r \\times k} $. Here, $ r $ is the rank of the decomposition, and $ r \\ll \\min(d, k) $.\n",
    "\n",
    "**3. Updated Weight Matrix**\n",
    "\n",
    "The updated weight matrix $ W' $ is then given by:\n",
    "\n",
    "$$\n",
    "W' = W + \\Delta W = W + A B\n",
    "$$\n",
    "\n",
    "**4. Training**\n",
    "\n",
    "During training, only the matrices $ A $ and $ B $ are updated, while the original weight matrix $ W $ remains fixed. This significantly reduces the number of trainable parameters from $ d \\times k $ to $ r \\times (d + k) $.\n",
    "\n",
    "**5. Forward Pass**\n",
    "\n",
    "During the forward pass, the input $ x $ is transformed using the updated weight matrix:\n",
    "\n",
    "$$\n",
    "y = W' x = (W + A B) x\n",
    "$$\n",
    "\n",
    "This can be computed efficiently by first computing $ B x $ and then $ A (B x) $.\n",
    "\n",
    "By using low-rank matrices $ A $ and $ B $ the number of trainable parameters are reduced and so it reduces the computational and memory overhead associated with fine-tuning large models This makes it a practical approach for adapting pre-trained models to specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "          Embedding h                    \n",
    "               ‚ñ≤                         \n",
    "               |                         \n",
    "        +------+------+                  \n",
    "        |             |                  \n",
    "        |      +      |                  \n",
    "        |             |                  \n",
    "        ‚ñ≤             ‚ñ≤                  \n",
    "+-----------------+   +----------------+ \n",
    "|  Pretrained     |   |  Weight Update | \n",
    "|  Weights  W     |   |  ŒîW            | \n",
    "| (Frozen)        |   | (Trainable)    | \n",
    "+-----------------+   +----------------+ \n",
    "        ‚ñ≤             ‚ñ≤                  \n",
    "        |             |                  \n",
    "        +------+------+                  \n",
    "               |                         \n",
    "               ‚ñ≤                         \n",
    "            Inputs x                     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets dive in to a practical example. For this, we will finetune a \"distilbert-base-uncased\" model from hugging face on the imdb dataset for sequence classification.\n",
    "\n",
    "For this we need to import everything we will use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Restart the notebook at this point to make shure we have no models or things left in the ram.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our model name and dataset name. You can switch them to different ones here, if you like to experiment with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# dataset\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the dataset and shuffle it. We implement a seed to keep the shuffle everytime we shuffle the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TASK\n",
    "\n",
    "Inspect the dataset: \n",
    "1. Print the dataset structure\n",
    "2. Print two examples of the dataset with the sentences and the lables. \n",
    "3. Use matplotlib to plot the distribution of sentiment labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "### IMPLEMENT YOUR SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the tokenizer for our model using the `AutoTokenizer` from HF. After that we tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a proper training, we split the dataset in train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "# to reduce training time you can use the following to select only a subset of the dataset\n",
    "TRAIN_SUBSET_SIZE = 500\n",
    "TEST_SUBSET_SIZE = 100\n",
    "SEED = 42\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=SEED).select(range(TRAIN_SUBSET_SIZE))\n",
    "test_dataset = test_dataset.shuffle(seed=SEED).select(range(TEST_SUBSET_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# make a train validation split from the train_dataset\n",
    "split_dataset = train_dataset.train_test_split(\n",
    "    test_size=0.2,\n",
    "    stratify_by_column=\"label\",  # Ensure same label distribution\n",
    "    seed=SEED  \n",
    ")\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the model using the `AutoModel` class `AutoModelForSequenceClassification`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now observe the model architecture with the `summary` function from the `torchinfo` framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start defining our LoRA adapter. For this, we define a lora configuration. \n",
    "\n",
    "In the configuration we need to define the following:\n",
    "- task_type: The task we want to train our model\n",
    "- r: The rank of the LoRA adapter ($\\Delta W = A B$). We have more trainable parameters when the rank is bigger. \n",
    "- lora_alpha: The alpha is the scaling factor. This is calculated together with the rank: $$\\Delta W = \\frac{\\alpha}{r} AB$$\n",
    "- lora_dropout: This defines the amount of dropout we want to use to make the training more robust. \n",
    "- target_modules: Defines the layers we want to add our adapter to. In this case, we target the attention layers for the LoRA. \n",
    "\n",
    "After the configuration we need to add the adapter to the model. Now we can observe the number of trainable paramters vs. the number of non trainable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,   # Sequence classification\n",
    "    r=8,                          # Rank\n",
    "    lora_alpha=32,                # Scaling factor\n",
    "    lora_dropout=0.1,             # Dropout\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]  # Target attention layers for LoRA\n",
    ")\n",
    "\n",
    "# you can experminet with different target modules\n",
    "# target_modules=[\n",
    "#     \"q_lin\", \"k_lin\", \"v_lin\",  # Self-attention projections\n",
    "#     \"out_lin\",  # Output projection of self-attention\n",
    "#     \"ffn.lin1\", \"ffn.lin2\"  # Feed-forward network layers\n",
    "# ]\n",
    "\n",
    "# add adapter to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we define the training arguments and the trainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_imdb\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all that preparation we can now train the adapter on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(\"./lora_sentiment_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the loss curves per epoch from our training to observe potential overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract training logs\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Prepare lists to store epoch-wise metrics\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "epochs = []\n",
    "\n",
    "for log in log_history:\n",
    "    if 'loss' in log and 'epoch' in log:\n",
    "        train_loss.append(log['loss'])\n",
    "        epochs.append(log['epoch'])\n",
    "    elif 'eval_loss' in log and 'epoch' in log:\n",
    "        eval_loss.append(log['eval_loss'])\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs[:len(train_loss)], train_loss, label=\"Training Loss\")\n",
    "plt.plot(epochs[:len(eval_loss)], eval_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving we can observ how good the model performs on our test dataset using the hugging face evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(\n",
    "    eval_dataset = test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TASK\n",
    "\n",
    "Use the trained model to classify the following example sentences.\n",
    "\n",
    "1. Load the model and tokenizer from the save directory.\n",
    "2. Create a pipeline.\n",
    "3. Classify the example sentences. \n",
    "4. Print out the results and format the output. \n",
    "\n",
    "\n",
    "The output per example should look something like this: \n",
    "```\n",
    "--------------------------------------------------\n",
    "Text: I am neutral\n",
    "Sentiment: Negative\n",
    "Confidence: 0.5093\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_classify = [\n",
    "    \"I love this\", \n",
    "    \"I hate this\", \n",
    "    \"I am neutral\", \n",
    "    \"This movie was really bad. I would not recommend it.\", \n",
    "    \"\\n\",\n",
    "    \"This movie was really bad. I would not recommend it.\",\n",
    "    \"I loved this movie! The acting was amazing.\",\n",
    "    \"This is no art-house film, it's mainstream entertainment. <br /><br />Lot's of beautiful people, t&a, and action. I found it very entertaining. It's not supposed to be intellectually stimulating, it's a fun film to watch! Jesse and Chace are funny too, which is just gravy. Definitely worth a rental.<br /><br />So in summary, I'd recommend checking it out for a little Friday night entertainment with the boys or even your girl (if she likes to see other girls get it on!)<br /><br />The villains are good too. Vinnie, Corey Large, the hatian guy from Heroes. Very nasty villains.\",\n",
    "    \"This film seemed way too long even at only 75 minutes. The problem with jungle horror films is that there is always way too much footage of people walking (through the jungle, up a rocky cliff, near a river or lake) to pad out the running time. The film is worth seeing for the laughable and naked native zombie with big bulging, bloody eyes which is always accompanied on the soundtrack with heavy breathing and lots of reverb. Eurotrash fans will be plenty entertained by the bad English dubbing, gratuitous female flesh and very silly makeup jobs on the monster and native extras. For a zombie/cannibal flick this was pretty light on the gore but then I probably didn't see an uncut version.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPLEMENT YOUR SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Instruction-tuning a LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning the basics we now want to use LoRa to finetune a LLM on Instruction-tuning. \n",
    "\n",
    "Wikipedia definition of Instruction-tuning: \n",
    "> \"Using \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be \"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.\" [LLMs definition Wikipedia](https://en.wikipedia.org/wiki/Large_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Restart the notebook at this point to make shure we have no models or things left in the ram.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model -> https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# dataset -> https://huggingface.co/datasets/tatsu-lab/alpaca\n",
    "dataset_name = \"tatsu-lab/alpaca\"\n",
    "\n",
    "# set a seed\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "dataset = dataset.shuffle(seed=SEED)\n",
    "\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a instruction-tuning, we need to create instructions from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_alpaca(example):\n",
    "    prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n\"\n",
    "    if example['input']:\n",
    "        prompt += f\"### Input:\\n{example['input']}\\n\\n\"\n",
    "    prompt += f\"### Response:\\n{example['output']}\"\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the map function to apply the instruction format to our dataset\n",
    "train_dataset = dataset.map(format_alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first sample from the dataset \n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.05, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset creation we need to convert the text in the token-space for the LLM to understand using the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = split_dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=tokenized_dataset[\"train\"]\n",
    "test_dataset=tokenized_dataset[\"test\"]\n",
    "\n",
    "# reducing the dataset size\n",
    "TRAIN_SUBSET_SIZE = 50\n",
    "TEST_SUBSET_SIZE = 10\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=SEED).select(range(TRAIN_SUBSET_SIZE))\n",
    "test_dataset = test_dataset.shuffle(seed=SEED).select(range(TEST_SUBSET_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load the actual model and defining the LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # typical for transformer-based models\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the adapter to the transformer\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# observe the trainable params\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Data Collator -> https://huggingface.co/docs/transformers/main_classes/data_collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_tinyllama_lora_alpaca\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    # gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    fp16=False,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    push_to_hub=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(\"./my_tinyllama_lora_alpaca_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract training logs\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Prepare lists to store epoch-wise metrics\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "epochs = []\n",
    "\n",
    "for log in log_history:\n",
    "    if 'loss' in log and 'epoch' in log:\n",
    "        train_loss.append(log['loss'])\n",
    "        epochs.append(log['epoch'])\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs[:len(train_loss)], train_loss, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(\n",
    "    eval_dataset = test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Using your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only saved the LoRA adapter to our system we need to load the basemodel first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model -> https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# set a seed\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load your LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./my_tinyllama_lora_alpaca_model\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your device we want to move the model to the GPU or CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the theory of relativity in simple terms.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated response:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats you are now able to fine-tune your own models. Feel free to dive in, there is a lot learn!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COSINE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
